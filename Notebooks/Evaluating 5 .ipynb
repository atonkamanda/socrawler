{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Now that we have our model trained we are able to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the ALBERT model to convert the query into a vector\n",
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "\n",
    "albert_tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\") \n",
    "\n",
    "from transformers import  AlbertConfig\n",
    "\n",
    "config = AlbertConfig.from_pretrained('./albert', output_hidden_states=True)\n",
    "\n",
    "model = TFAlbertModel.from_pretrained('./albert', config=config,  from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bk3tN1pe-UEn",
    "outputId": "6ac98423-547e-4fec-dbe0-ee75b19b0025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./final\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(modified_transformer=modified_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hibHE99y-UEq"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldqEEjZS-UEs"
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    \n",
    "    enc_padding_mask = create_masks(inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = modified_transformer(inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask)\n",
    "        loss = loss_function(tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, modified_transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, modified_transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferring encoder weights from the transformer trained on translation of functions to english to the encoders of \n",
    "# the modified transformer \n",
    "modified_transformer.layers[0].set_weights(transformer.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the encoder layers and train the LSTM and dense layers only\n",
    "# Then train the entire architecture by setting trainable to true and beginning training\n",
    "modified_transformer.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings for the functions without docstrings\n",
    "def generate_embeddings(inp_sentence):\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0) # Create input with batch size 1\n",
    "    enc_padding_mask, = create_masks(encoder_input) # Create encoder mask\n",
    "    predictions = modified_transformer(encoder_input,False,enc_padding_mask) # Get the function vector\n",
    "    return predictions[0].numpy() # Convert the function vector to numpy array and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_docstrings = pd.read_csv('without_docstrings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>19</td>\n",
       "      <td>def __init__(self, *leafs, **edges):\\r\\n    se...</td>\n",
       "      <td>init self leafs edges self edges edges self le...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__eq__</td>\n",
       "      <td>23</td>\n",
       "      <td>def __eq__(self, other):\\r\\n    if isinstance(...</td>\n",
       "      <td>eq self other if isinstance other node return ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__repr__</td>\n",
       "      <td>29</td>\n",
       "      <td>def __repr__(self):\\r\\n    return 'Node&lt;leafs=...</td>\n",
       "      <td>repr self return node leafs edges format self ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>_isCapitalized</td>\n",
       "      <td>170</td>\n",
       "      <td>@staticmethod\\r\\ndef _isCapitalized(token):\\r\\...</td>\n",
       "      <td>iscapitalized token return len token 1 and tok...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>_isCapitalizeD</td>\n",
       "      <td>175</td>\n",
       "      <td>@staticmethod\\r\\ndef _isCapitalizeD(last, toke...</td>\n",
       "      <td>iscapitalized last token return last and len t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         nwo                       path   function_name  lineno  \\\n",
       "0           0  fnl/libfnl  src/fnl/nlp/dictionary.py        __init__      19   \n",
       "1           1  fnl/libfnl  src/fnl/nlp/dictionary.py          __eq__      23   \n",
       "2           2  fnl/libfnl  src/fnl/nlp/dictionary.py        __repr__      29   \n",
       "3          10  fnl/libfnl  src/fnl/nlp/dictionary.py  _isCapitalized     170   \n",
       "4          11  fnl/libfnl  src/fnl/nlp/dictionary.py  _isCapitalizeD     175   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def __init__(self, *leafs, **edges):\\r\\n    se...   \n",
       "1  def __eq__(self, other):\\r\\n    if isinstance(...   \n",
       "2  def __repr__(self):\\r\\n    return 'Node<leafs=...   \n",
       "3  @staticmethod\\r\\ndef _isCapitalized(token):\\r\\...   \n",
       "4  @staticmethod\\r\\ndef _isCapitalizeD(last, toke...   \n",
       "\n",
       "                                     function_tokens docstring_tokens  \\\n",
       "0  init self leafs edges self edges edges self le...              NaN   \n",
       "1  eq self other if isinstance other node return ...              NaN   \n",
       "2  repr self return node leafs edges format self ...              NaN   \n",
       "3  iscapitalized token return len token 1 and tok...              NaN   \n",
       "4  iscapitalized last token return last and len t...              NaN   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "1  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "2  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "3  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "4  https://github.com/fnl/libfnl/blob/master/src/...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_docstrings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_vector = []\n",
    "for i in range(len(without_docstrings['function_tokens'].values)):\n",
    "    inp = tokenizer_func.encode(test_df['function_tokens'][i]).ids # Encode the function tokens\n",
    "    func_vector.append(generate_embeddings(inp)) # Store the list of function vectors\n",
    "\n",
    "import csv\n",
    "# Store the fucntion vectors in a .tsv file\n",
    "with open(\"func_vectors.tsv\",\"w+\",newline='') as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter='\\t')\n",
    "    csvWriter.writerows(func_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "# We initliaze values to use Hierarchial Navigable Small Worlds (hnsw) and Cosine similarity as the distance metric\n",
    "search_index = nmslib.init(method='hnsw', space='cosinesimil') \n",
    "e = np.loadtxt('func_vectors.tsv',delimiter='\\t') # Load our saved fucntion vectors\n",
    "search_index.addDataPointBatch(e) \n",
    "search_index.createIndex(print_progress=True) # Create our search indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    e = albert_tokenizer.encode(query, max_length=512) # Use our trained ALBERT model to generate query vector\n",
    "    input = tf.constant(e)[None, :]  # Batch size 1 \n",
    "    output = model(input)\n",
    "    v = [0]*768\n",
    "    for i in range(1, len(input[0])-1):\n",
    "        v = v + output[0][0][i].numpy()  \n",
    "    emb = v/len(input[0]) # Generate query vector\n",
    "    \n",
    "    # Search five nearest neighbours, their index value and cosine distances are returned\n",
    "    idxs, dists = search_index.knnQuery(emb, k=5) \n",
    "\n",
    "    # Function details for the index value returned are extracted and printed\n",
    "    for idx, dist in zip(idxs, dists):\n",
    "            code = without_docstrings['original_function'][idx]  \n",
    "            url = without_docstrings['url'][idx]\n",
    "            print(f'cosine dist:{dist:.4f} \\n {url}  \\n {code} \\n---------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index.saveIndex('./final.nmslib') # Save the search indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index.loadIndex('./final.nmslib') # Load the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    
      "\n",
      "cosine dist:0.0210  \n",
      "https://github.com/cisco-sas/kitty/blob/master/kitty/monitors/base.py#L66 \n",
      " def _is_alive(self):\r\n",
      "         if self.monitor_thread is not None:\r\n",
      "        if self.monitor_thread.is_alive():\r\n",
      "            return True\r\n",
      "    return False\r\n",
      " \n",
      "---------------------------------\n",
      "\n",
    
     ]
    }
   ],
   "source": [
    "search('live process checker') # Search"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
