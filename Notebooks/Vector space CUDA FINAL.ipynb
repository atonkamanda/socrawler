{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4db27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8696db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('codegit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32e050e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>def train(train_dir, model_save_path=None, n_n...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>def predict(X_img_path, knn_clf=None, model_pa...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>def show_prediction_labels_on_image(img_path, ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>def _trim_css_to_bounds(css, image_shape):\\n  ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457456</th>\n",
       "      <td>457456</td>\n",
       "      <td>def show_version(self):\\n        \"\"\" custom co...</td>\n",
       "      <td>https://github.com/xnuinside/clifier/blob/3d70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457457</th>\n",
       "      <td>457457</td>\n",
       "      <td>def check_path_action(self):\\n        \"\"\" cust...</td>\n",
       "      <td>https://github.com/xnuinside/clifier/blob/3d70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457458</th>\n",
       "      <td>457458</td>\n",
       "      <td>def new_user(yaml_path):\\n    '''\\n    Return ...</td>\n",
       "      <td>https://github.com/tklovett/PyShirtsIO/blob/ff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457459</th>\n",
       "      <td>457459</td>\n",
       "      <td>def _AddPropertiesForExtensions(descriptor, cl...</td>\n",
       "      <td>https://github.com/ibelie/typy/blob/3616845fb9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457460</th>\n",
       "      <td>457460</td>\n",
       "      <td>def _InternalUnpackAny(msg):\\n  \"\"\"Unpacks Any...</td>\n",
       "      <td>https://github.com/ibelie/typy/blob/3616845fb9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457461 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               code  \\\n",
       "0                0  def train(train_dir, model_save_path=None, n_n...   \n",
       "1                1  def predict(X_img_path, knn_clf=None, model_pa...   \n",
       "2                2  def show_prediction_labels_on_image(img_path, ...   \n",
       "3                3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   \n",
       "4                4  def _trim_css_to_bounds(css, image_shape):\\n  ...   \n",
       "...            ...                                                ...   \n",
       "457456      457456  def show_version(self):\\n        \"\"\" custom co...   \n",
       "457457      457457  def check_path_action(self):\\n        \"\"\" cust...   \n",
       "457458      457458  def new_user(yaml_path):\\n    '''\\n    Return ...   \n",
       "457459      457459  def _AddPropertiesForExtensions(descriptor, cl...   \n",
       "457460      457460  def _InternalUnpackAny(msg):\\n  \"\"\"Unpacks Any...   \n",
       "\n",
       "                                                      url  \n",
       "0       https://github.com/ageitgey/face_recognition/b...  \n",
       "1       https://github.com/ageitgey/face_recognition/b...  \n",
       "2       https://github.com/ageitgey/face_recognition/b...  \n",
       "3       https://github.com/ageitgey/face_recognition/b...  \n",
       "4       https://github.com/ageitgey/face_recognition/b...  \n",
       "...                                                   ...  \n",
       "457456  https://github.com/xnuinside/clifier/blob/3d70...  \n",
       "457457  https://github.com/xnuinside/clifier/blob/3d70...  \n",
       "457458  https://github.com/tklovett/PyShirtsIO/blob/ff...  \n",
       "457459  https://github.com/ibelie/typy/blob/3616845fb9...  \n",
       "457460  https://github.com/ibelie/typy/blob/3616845fb9...  \n",
       "\n",
       "[457461 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50bc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d783f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def train(train_dir, model_save_path=None, n_n...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def predict(X_img_path, knn_clf=None, model_pa...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def show_prediction_labels_on_image(img_path, ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def _trim_css_to_bounds(css, image_shape):\\n  ...</td>\n",
       "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457456</th>\n",
       "      <td>def show_version(self):\\n        \"\"\" custom co...</td>\n",
       "      <td>https://github.com/xnuinside/clifier/blob/3d70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457457</th>\n",
       "      <td>def check_path_action(self):\\n        \"\"\" cust...</td>\n",
       "      <td>https://github.com/xnuinside/clifier/blob/3d70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457458</th>\n",
       "      <td>def new_user(yaml_path):\\n    '''\\n    Return ...</td>\n",
       "      <td>https://github.com/tklovett/PyShirtsIO/blob/ff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457459</th>\n",
       "      <td>def _AddPropertiesForExtensions(descriptor, cl...</td>\n",
       "      <td>https://github.com/ibelie/typy/blob/3616845fb9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457460</th>\n",
       "      <td>def _InternalUnpackAny(msg):\\n  \"\"\"Unpacks Any...</td>\n",
       "      <td>https://github.com/ibelie/typy/blob/3616845fb9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     code  \\\n",
       "0       def train(train_dir, model_save_path=None, n_n...   \n",
       "1       def predict(X_img_path, knn_clf=None, model_pa...   \n",
       "2       def show_prediction_labels_on_image(img_path, ...   \n",
       "3       def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   \n",
       "4       def _trim_css_to_bounds(css, image_shape):\\n  ...   \n",
       "...                                                   ...   \n",
       "457456  def show_version(self):\\n        \"\"\" custom co...   \n",
       "457457  def check_path_action(self):\\n        \"\"\" cust...   \n",
       "457458  def new_user(yaml_path):\\n    '''\\n    Return ...   \n",
       "457459  def _AddPropertiesForExtensions(descriptor, cl...   \n",
       "457460  def _InternalUnpackAny(msg):\\n  \"\"\"Unpacks Any...   \n",
       "\n",
       "                                                      url  \n",
       "0       https://github.com/ageitgey/face_recognition/b...  \n",
       "1       https://github.com/ageitgey/face_recognition/b...  \n",
       "2       https://github.com/ageitgey/face_recognition/b...  \n",
       "3       https://github.com/ageitgey/face_recognition/b...  \n",
       "4       https://github.com/ageitgey/face_recognition/b...  \n",
       "...                                                   ...  \n",
       "457456  https://github.com/xnuinside/clifier/blob/3d70...  \n",
       "457457  https://github.com/xnuinside/clifier/blob/3d70...  \n",
       "457458  https://github.com/tklovett/PyShirtsIO/blob/ff...  \n",
       "457459  https://github.com/ibelie/typy/blob/3616845fb9...  \n",
       "457460  https://github.com/ibelie/typy/blob/3616845fb9...  \n",
       "\n",
       "[457461 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd26a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"./python_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14116f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0fc98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf66246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26340f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_concat = vector_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8521fbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([234853, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_concat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "526a9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed6c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([234854, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 690.00 MiB (GPU 0; 3.00 GiB total capacity; 1.81 GiB already allocated; 0 bytes free; 1.87 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-b16c012e4dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcode_concat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 690.00 MiB (GPU 0; 3.00 GiB total capacity; 1.81 GiB already allocated; 0 bytes free; 1.87 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for i in range(234853,300000):\n",
    "    with torch.no_grad():\n",
    "        new_vec=model(tokenizer(df[\"code\"][i],padding=True, truncation=True,return_tensors='pt')['input_ids'].cuda())[1]\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        code_concat=torch.cat((code_concat.cuda(),new_vec.cuda()),0)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(code_concat.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59ef8e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f175be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def delete(self, reason=\\'\\'):\\n        \"\"\"Deletes the event chatroom and if necessary the chatroom, too.\\n\\n        :param reason: reason for the deletion\\n        :return: True if the associated chatroom was also\\n                 deleted, otherwise False\\n        \"\"\"\\n        db.session.delete(self)\\n        db.session.flush()\\n        if not self.chatroom.events:\\n            db.session.delete(self.chatroom)\\n            db.session.flush()\\n            delete_room(self.chatroom, reason)\\n            return True\\n        return False'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"code\"][234852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a82cfaf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def delete(self, reason=''):\n",
      "        \"\"\"Deletes the event chatroom and if necessary the chatroom, too.\n",
      "\n",
      "        :param reason: reason for the deletion\n",
      "        :return: True if the associated chatroom was also\n",
      "                 deleted, otherwise False\n",
      "        \"\"\"\n",
      "        db.session.delete(self)\n",
      "        db.session.flush()\n",
      "        if not self.chatroom.events:\n",
      "            db.session.delete(self.chatroom)\n",
      "            db.session.flush()\n",
      "            delete_room(self.chatroom, reason)\n",
      "            return True\n",
      "        return False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    query = \"  Deletes the event chatroom and if necessary the chatroom, too.\"\n",
    "    query_vec = model(tokenizer(query,return_tensors='pt')['input_ids'].cuda())[1]\n",
    "    scores=torch.einsum(\"ab,cb->ac\",query_vec.cuda(),code_concat.cuda())\n",
    "    scores=torch.softmax(scores,-1)\n",
    "    prediction =torch.argmax(scores)\n",
    "    print(df['code'][int(prediction)])\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82d8c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "values ,indices = scores.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afa692d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba99580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999678134918213,\n",
       " 9.499352017883211e-06,\n",
       " 8.922521374188364e-06,\n",
       " 6.698330707877176e-06,\n",
       " 3.159749439873849e-06,\n",
       " 2.80609719993663e-06,\n",
       " 2.156397158614709e-07,\n",
       " 1.5499392702622572e-07,\n",
       " 1.48923220422148e-07,\n",
       " 1.1381721520820065e-07]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fddc3810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[234852, 136263, 126206, 183315,  53058, 136291,  40587, 136323,  42526,\n",
       "          54777]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639533cd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eee66c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec = model(tokenizer(df[\"code\"][174],padding=True, truncation=True,return_tensors='pt')['input_ids'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f69d7",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dd769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_concat=torch.cat((code_concat,new_vec),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d16b0a",
   "metadata": {},
   "source": [
    "# Saving vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5a46cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(code_concat, 'vector_space_234.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d054c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = torch.load('vector_space_234853.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc25c8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def saveAsSequenceFile(self, path, compressionCodecClass=None):\\n        \"\"\"\\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\\n        RDD\\'s key and value types. The mechanism is as follows:\\n\\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\\n            2. Keys and values of this Java RDD are converted to Writables and written out.\\n\\n        :param path: path to sequence file\\n        :param compressionCodecClass: (None by default)\\n        \"\"\"\\n        pickledRDD = self._pickled()\\n        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\\n                                                   path, compressionCodecClass)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"save as sequence file\"\n",
    "query_vec = model(tokenizer(query,return_tensors='pt')['input_ids'])[1]\n",
    "scores=torch.einsum(\"ab,cb->ac\",query_vec,vector_space)\n",
    "scores=torch.softmax(scores,-1)\n",
    "prediction =torch.argmax(scores)\n",
    "df['code'][int(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5e8f159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.9883e-05, 6.9719e-02, 1.4829e-04, 3.2514e-08, 1.5014e-04, 3.3651e-11,\n",
       "         7.3051e-03, 1.0486e-05, 8.4265e-05, 1.9627e-08, 9.9114e-10, 1.7181e-11,\n",
       "         7.6967e-07, 2.3420e-07, 7.5435e-05, 1.6278e-05, 1.2505e-06, 1.5227e-02,\n",
       "         3.7393e-07, 1.2441e-04, 3.5281e-07, 4.1157e-04, 3.9695e-07, 2.1586e-05,\n",
       "         2.2509e-07, 9.9083e-06, 1.1952e-03, 5.1477e-10, 4.5141e-05, 2.5419e-05,\n",
       "         1.7339e-06, 1.0706e-02, 6.4896e-07, 5.7015e-05, 2.3450e-08, 1.5728e-02,\n",
       "         1.4556e-02, 1.6688e-04, 3.7909e-02, 6.3608e-04, 1.0445e-05, 1.3230e-06,\n",
       "         2.7618e-01, 2.2660e-01, 1.2301e-02, 2.0655e-03, 2.3229e-06, 2.9770e-01,\n",
       "         5.3537e-03, 6.6324e-04, 1.0583e-08, 2.9835e-07, 4.3689e-08, 8.6893e-08,\n",
       "         7.0633e-04, 3.9256e-06, 5.8707e-07, 2.4961e-08, 3.5754e-08, 3.9828e-03]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c007fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0396,  0.0321, -0.1320,  ...,  0.1742, -0.2643,  0.3543],\n",
       "        [ 0.1358, -0.3192,  0.1960,  ..., -0.1715,  0.1855,  0.5175],\n",
       "        [-0.2785,  0.4911,  0.2407,  ..., -0.1964,  0.3436,  0.1469],\n",
       "        ...,\n",
       "        [-0.0553, -0.0858, -0.1548,  ..., -0.4652,  0.4013,  0.0755],\n",
       "        [ 0.1154, -0.4007, -0.2057,  ...,  0.3505, -0.2217,  0.5642],\n",
       "        [ 0.4179, -0.4487, -0.6455,  ..., -0.0456,  0.4984,  0.2423]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42eb9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a2eac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ad3c832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4b81792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715c5619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1,2,3]\n",
    "my_list = sorted(test, reverse=True)\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77728180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codebert",
   "language": "python",
   "name": "codebert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
